{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessor import Preprocessor\n",
    "\n",
    "#FilterReview\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import os.path\n",
    "\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model_filename = 'model/DocVec'\n",
    "        print('모델을 불러오는중...')\n",
    "        self.gnb = pickle.load(open('model/finalized_model_gbn.sav', 'rb'))\n",
    "            \n",
    "    def input_X(self, para):\n",
    "        pre = Preprocessor()\n",
    "        print(para)\n",
    "        pre.setPara(para)\n",
    "        print(pre.sentences)\n",
    "        self.X = pre.transVector()\n",
    "    def input_Y(self, result):\n",
    "        self.Y = result\n",
    "        \n",
    "    def readCSV(self,filename,tag=True):\n",
    "        self.data = pd.read_csv(filename,engine='python')\n",
    "        self.X_content = self.data['sentence'].values\n",
    "        self.input_X(self.X_content)\n",
    "        if tag == True:\n",
    "            self.input_Y(self.data['label'].values)\n",
    "        \n",
    "    def trainModel(self,ts):\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.Y,test_size=ts)\n",
    "        self.gnb = GaussianNB()\n",
    "        self.y_pred = self.gnb.fit(self.X_train, self.y_train).predict(self.X_test)\n",
    "\n",
    "        # 테스트 socre 확인\n",
    "        print(\"Number of right labeled points out of a total %d points : %.2f %%\"\n",
    "      % (len(self.y_test),100*(len(self.y_test)-(self.y_test != self.y_pred).sum())/len(self.y_test)))\n",
    "        \n",
    "        pickle.dump(self.gnb, open(self.model_filename+'.sav', 'wb'))\n",
    "        isSetModel = True\n",
    "    def predict(self):\n",
    "        self.y_pred = self.gnb.predict(self.X)\n",
    "        self.df = pd.DataFrame({'ps_label':self.y_pred, 'sentence':self.X_content})\n",
    "        return self.y_pred\n",
    "    def writePredictionResult(self, filename):\n",
    "        self.df.to_csv(filename,encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델을 불러오는중...\n",
      "['아 더빙.. 진짜 짜증나네요 목소리' '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나'\n",
      " '너무재밓었다그래서보는것을추천한다' ... '그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다'\n",
      " '절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네' '마무리는 또 왜이래']\n",
      "['아 더빙.. 진짜 짜증나네요 목소리' '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나'\n",
      " '너무재밓었다그래서보는것을추천한다' ... '그림도 좋고 완성도도 높았지만... 보는 내내 불안하게 만든다'\n",
      " '절대 봐서는 안 될 영화.. 재미도 없고 기분만 잡치고.. 한 세트장에서 다 해먹네' '마무리는 또 왜이래']\n"
     ]
    }
   ],
   "source": [
    "sa = SentimentAnalysis()\n",
    "sa.readCSV(tag=True,filename='MovieReview/MovieReview.csv')\n",
    "sa.trainModel(ts=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SentimentAnalysis()\n",
    "sa.readCSV(tag=False,filename='Review.csv')\n",
    "sa.predict()\n",
    "sa.writePredictionResult(filename='ReviewScore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "from konlpy.tag import Twitter\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "    return data\n",
    "\n",
    "def tokenize(doc):\n",
    "  # norm, stem은 optional\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "\n",
    "# 테스트 데이터를 읽기\n",
    "train_data = read_data('data/ratings_train.txt')\n",
    "test_data = read_data('data/ratings_test.txt')\n",
    "\n",
    "# 형태소 분류\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:100]]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:100]]\n",
    "\n",
    "# doc2vec 에서 필요한 데이터 형식으로 변경\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'words tags')\n",
    "tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]\n",
    "tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 700\n",
    "# load train data\n",
    "#doc_vectorizer = Doc2Vec.load('model/doc2vec.model'+str(size))\n",
    "doc_vectorizer = Doc2Vec.load('modelByUijung/리뷰모음_train_랄라.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infer_vector   :   단어간의 유사도를 나타낸 값\n",
    "# 분류를 위한 피쳐 생성\n",
    "train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]  \n",
    "train_y = [doc.tags[0] for doc in tagged_train_docs]\n",
    "test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]\n",
    "test_y = [doc.tags[0] for doc in tagged_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -54.260757  ,   -2.853084  ,  -34.34145   ,   50.7547    ,\n",
       "        194.22871   ,  -60.40188   ,    2.4805784 ,   11.578267  ,\n",
       "         84.84678   ,   33.045166  ,  -79.88328   ,  -39.655052  ,\n",
       "       -134.8376    , -127.83568   ,  -31.927078  ,   -3.9180186 ,\n",
       "         42.61494   ,   63.66775   ,   69.438576  ,  -29.77148   ,\n",
       "         68.76532   ,  -12.632942  ,  -77.55052   ,  -60.610287  ,\n",
       "         83.53197   ,   26.991373  ,  -24.95431   ,  -35.36142   ,\n",
       "        -27.655022  ,   56.669495  ,  -80.353004  ,   38.236763  ,\n",
       "         -0.5948937 , -161.06145   ,   36.570663  ,   62.828297  ,\n",
       "         39.142353  ,   41.932552  ,  -12.516527  ,  -90.4508    ,\n",
       "        -74.75085   ,   66.50075   ,   98.31159   ,  -36.65413   ,\n",
       "       -141.14304   ,   45.98348   ,   -5.867824  ,  -19.85603   ,\n",
       "        -25.722172  ,   53.330933  , -100.416115  ,   47.09637   ,\n",
       "         81.93322   ,   85.50847   , -120.40288   ,  -36.186207  ,\n",
       "       -159.84073   ,  -31.598936  ,   46.842815  , -142.52687   ,\n",
       "         78.64796   ,    9.072585  ,   62.0227    ,  -57.224606  ,\n",
       "        -56.45883   ,  -56.92172   ,  -89.13677   ,    1.6268618 ,\n",
       "        111.2797    ,   -1.9811499 ,   21.085201  ,  -75.45803   ,\n",
       "        121.80699   ,   44.658802  ,  -17.986036  ,  -25.718683  ,\n",
       "        -73.453896  ,    9.846356  ,  -85.766495  ,   94.10148   ,\n",
       "         26.865076  ,  153.80562   ,  141.41998   ,   51.643524  ,\n",
       "        -64.78836   ,  -20.374943  ,  -45.614513  , -146.89792   ,\n",
       "        -26.44672   ,  -13.555101  ,  -58.625835  ,  122.14639   ,\n",
       "         -5.967999  ,  -90.18865   ,   58.537235  ,   28.1167    ,\n",
       "         66.59291   ,   62.287235  ,   87.45779   ,  -98.60949   ,\n",
       "         -7.6143084 ,   -0.666981  ,   12.230078  ,  -85.36666   ,\n",
       "        -26.153845  ,  144.63274   ,   68.31426   ,   14.086372  ,\n",
       "        -98.45581   ,   37.450783  ,   60.50957   ,   98.38766   ,\n",
       "        -77.744316  ,   21.165874  ,  -78.148254  ,  -71.71867   ,\n",
       "        -91.001976  ,  102.44544   ,   74.3981    ,  -56.045315  ,\n",
       "        -36.756046  ,  105.12745   ,  -39.626865  ,   91.35687   ,\n",
       "        102.55798   ,  -61.493732  ,   53.333504  ,  -25.206718  ,\n",
       "         88.499176  ,  -83.237816  ,   58.08026   , -103.379585  ,\n",
       "         41.133953  ,  -19.199888  ,   12.241446  ,   62.22932   ,\n",
       "         -6.6132636 ,  116.341064  ,  138.64867   ,  -74.83941   ,\n",
       "         45.47053   ,  221.7965    ,   72.18241   ,  113.404434  ,\n",
       "        129.23361   ,  -87.93212   ,   52.982872  ,   46.18569   ,\n",
       "        -47.32118   ,    0.55757827,  138.6792    ,  121.04612   ,\n",
       "        -43.3019    ,    1.6029909 , -140.72507   ,  -15.061892  ,\n",
       "         -6.15517   ,  -32.115578  ,  201.4643    ,   24.151144  ,\n",
       "        -34.915474  ,  109.38265   ,  -20.931704  ,  -19.595457  ,\n",
       "         54.8808    ,   -7.782279  ,   10.822865  ,   -4.235632  ,\n",
       "        -94.399765  ,   27.390144  , -113.066795  , -135.05199   ,\n",
       "         68.13045   ,   80.15766   ,   72.39812   ,  -63.52523   ,\n",
       "        -63.904427  ,  -46.820034  ,   45.427925  ,   70.6884    ,\n",
       "        -13.004179  ,  101.01336   ,  -54.499847  ,  -20.350931  ,\n",
       "        126.38989   ,  -60.982334  ,   47.351086  ,  169.76059   ,\n",
       "         94.37238   ,  -57.9313    ,  -82.224335  ,   10.546047  ,\n",
       "        -22.472252  ,  -88.34606   ,   71.92867   ,  -81.91176   ,\n",
       "         85.1262    ,  210.74419   ,   -2.1231737 ,  -13.487335  ,\n",
       "          4.197143  ,   32.50305   ,   41.977962  ,   45.273384  ,\n",
       "        -67.25438   ,   58.15518   ,   -7.2722416 ,   14.116962  ,\n",
       "        -97.754005  , -107.28152   ,  -70.38133   ,   64.25157   ,\n",
       "         37.659237  ,  -61.87938   ,  -94.56771   ,   51.016575  ,\n",
       "        164.86581   ,  -82.42411   ,    0.5716748 ,  -16.087969  ,\n",
       "        124.02751   , -161.05325   ,   82.3798    ,   88.68871   ,\n",
       "        -63.932396  ,   62.438557  ,  -18.362131  ,   72.363785  ,\n",
       "        191.87077   ,    5.8588805 , -136.8507    , -190.78302   ,\n",
       "        102.23702   ,  -71.160614  ,  -52.42804   ,  -76.72264   ,\n",
       "        161.79178   ,  -33.026672  ,   73.50714   ,  -44.728382  ,\n",
       "        -61.49084   ,  -55.08933   ,  -81.1329    ,   -1.7186618 ,\n",
       "         36.434185  ,   88.544785  ,   50.821003  ,  -82.63368   ,\n",
       "        -46.673218  ,   85.65305   ,  -38.75301   ,  -37.14441   ,\n",
       "         59.876823  ,  -27.792887  ,  -92.06133   ,  -43.724895  ,\n",
       "       -208.8035    ,   46.914494  ,  -32.545746  ,   85.05495   ,\n",
       "         37.768078  ,   46.224804  , -115.1433    ,  -71.21883   ,\n",
       "        -72.02532   ,  -82.37937   , -180.53499   ,  -87.831604  ,\n",
       "        -53.69418   ,   93.82232   ,   47.235733  ,  128.82944   ,\n",
       "          0.39269277,  117.96679   ,  -60.92547   ,  -40.16217   ,\n",
       "        -87.41703   ,  -17.06414   , -107.27378   , -108.09663   ,\n",
       "         26.426926  ,   -7.0148525 , -258.7653    ,   64.374084  ,\n",
       "          1.2961375 ,   32.816048  ,  -31.982773  , -111.85645   ,\n",
       "         15.3312235 ,  -33.29422   ,    4.0746903 , -191.00952   ,\n",
       "        -15.58282   ,   14.25309   ,  101.07372   ,  149.70267   ,\n",
       "         82.10679   ,   78.1951    ,   44.150097  ,   93.07231   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorizer.infer_vector(tagged_train_docs[0].words)*1000     #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 150000 points : 22232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nclassifier = nltk.NaiveBayesClassifier.train(train_xy) #Naive Bayes classifier 적용\\nprint(nltk.classify.accuracy(classifier, test_xy))\\n\\n\\n#classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\\nclassifier = LogisticRegression(random_state=1234)\\nclassifier.fit(train_x, train_y)\\n\\n# 테스트 socre 확인\\nprint( classifier.score(test_x, test_y) )\\n# 0.63904\\n\\n# save the model to disk\\nfilename = 'model/finalized_model.sav'\\npickle.dump(classifier, open(filename, 'wb'))\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "pred_y = gnb.fit(train_x, train_y).predict(test_x)\n",
    "\n",
    "# 테스트 socre 확인\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (len(train_x),(test_y != pred_y).sum()))\n",
    "# 0.85179\n",
    "    \n",
    "'''\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_xy) #Naive Bayes classifier 적용\n",
    "print(nltk.classify.accuracy(classifier, test_xy))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model/finalized_model_gbn_'+str(size)+'.sav'\n",
    "pickle.dump(gnb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state=1234)\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "# 테스트 socre 확인\n",
    "print( classifier.score(test_x, test_y) )\n",
    "# 0.63904\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'model/finalized_model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
